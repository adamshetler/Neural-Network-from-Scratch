{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #handwritten digits classification using multi-layer perceptron artificial neural network from scratch \n",
    "\n",
    "#this notebook will demonstrate how to create an artificial multi-layer perceptron neural network from scratch, making use of \n",
    "#only numpy for matrix computations, and train_test_split to split our dataset. We will use the sigmoid function as activation \n",
    "#function, our cost function will be an extension of the one used for logistic regression and we will use both batch and \n",
    "#stochastic gradient descent as our optimization alogorithms. We will then analyze how well our model generalizes to new data \n",
    "#using k-fold cross validation. The steps include:\n",
    "    #1.) create the network architecture\n",
    "    #2.) randomly initialize weights based on network architecutre\n",
    "    #3.) for each training example: feedforward then back propagation to get partial derivatives \n",
    "    #4.) update weights using gradient descent (batch/stochastic)\n",
    "    #5.) repeat steps 3 and 4 for a given number of epochs \n",
    "    #6.) use 5-fold cross validation to see how well our results will generalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTE: syntax for arrays vs. matrix multiplication\n",
    "# the * opperand does element-wise multiplication for arrays\n",
    "# the * opperand does matrix multiplication for matrices \n",
    "# the np.dot() command does matrix multiplication for both arrays and matrices \n",
    "# the np.multiply() command does element-wise multiplication for both arrays and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797L, 64L)\n",
      "(1797L,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#load dataset of handwritten digits from sklearn \n",
    "digits = datasets.load_digits()\n",
    "#num_rows=num training examples \n",
    "print digits['data'].shape\n",
    "#num_cols=num features\n",
    "print digits['target'].shape\n",
    "#number of classes = len(digits['target_names])\n",
    "print digits['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbdfc668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#example of what one of the images from the dataset looks like (8x8 pixels)\n",
    "plt.gray()\n",
    "plt.imshow(digits.images[25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define sigmoid function. This will be used as the activation function\n",
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define derivative of sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    #check if input is a matrix (need element-wise multiplication)\n",
    "    if type(z)==np.matrixlib.defmatrix.matrix:\n",
    "        return np.multiply(z,(1-z))\n",
    "    else:\n",
    "        return (z)*(1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize the network architecture. Inputs include:\n",
    "#       n_inputs = number of inputs in first layer \n",
    "#       num_hidden_units = list which contains the number of hidden units in each hidden layer\n",
    "#       num_hidden_layers = number of hidden layers\n",
    "#       n_outputs = number of units in the output layer\n",
    "#this function will return a list which contains each layer of the network as a vector of zeros\n",
    "def initializeNet(n_inputs,num_hidden_units,num_hidden_layers,n_outputs):\n",
    "    first_layer = np.zeros((n_inputs,1))\n",
    "    output_layer = np.zeros((n_outputs,1))\n",
    "    lis_hidden_layers = []\n",
    "    lis_hidden_layers.append(first_layer)\n",
    "    for i in range(num_hidden_layers):\n",
    "        hidden_layer = np.zeros((num_hidden_units[i],1))\n",
    "        lis_hidden_layers.append(hidden_layer)\n",
    "    lis_hidden_layers.append(output_layer)\n",
    "    return lis_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes as an input a network which should be the output of the function initializeNet()\n",
    "#this function randomly initializes the weight matrices used to get from one layer of the network to the next \n",
    "#the output will be a list which contains each theta matrix \n",
    "def initializeWeights(network):\n",
    "    num_thetas = len(network)-1\n",
    "    theta_list = []\n",
    "    for i in range(num_thetas):\n",
    "        theta = np.random.randn(len(network[i+1]),len(network[i])+1)\n",
    "        theta_list.append(theta)\n",
    "    return theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes as input:\n",
    "    #data_labels: the labels for a given dataset\n",
    "    #num_labels: a list containing each unique label ie. for handwritten digits [0,1,2,3,4,5,6,7,8,9]\n",
    "#this function will return:\n",
    "    #y_vector: a matrix with dimensions (num_labels)x(num training examples) where there will be a one indicating that index is \n",
    "#the correct label and a zero indicating that index is not the correct label \n",
    "def yLabels(data_labels,num_labels):\n",
    "    y_vector = np.zeros((len(num_labels),data_labels.shape[0]))\n",
    "    for i in range(data_labels.shape[0]):\n",
    "        for j in range(len(num_labels)):\n",
    "            if (data_labels[i]==num_labels[j]):\n",
    "                y_vector[j,i] = 1\n",
    "            else:\n",
    "                y_vector[j,i] = 0\n",
    "    return y_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function will update weights, theta by taking a single step of gradient descent \n",
    "def gradientDescent(theta,thetGrad,alpha):\n",
    "    for i in range(len(theta)):\n",
    "        theta[i]+= -(alpha*thetGrad[i])\n",
    "    return theta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes the training data, randomly initialized weights and the pre-determined architecture for a neural network\n",
    "#and performs for each training example: feedforward, then backpropagation to get the partial derivatives of the cost function\n",
    "#with repsect to each weight. The output of this function will be a list of arrays containing partial derivatives. This will \n",
    "#be used as the input for gradient descent which is used to update the network weights. \n",
    "#this function takes inputs:\n",
    "        # Xdata- array, train data we are feeding the neural net, \n",
    "        # ydata-train data labels \n",
    "        # thetas- list, contains randomly initialized weights \n",
    "#the output of the function:\n",
    "        # thetas = list, contains updated matrices of weights after 1 epoch\n",
    "def propagation(Xdata,ydata,thetas,alpha,lamb):\n",
    "    #number of examples \n",
    "    M = Xdata.shape[0]\n",
    "    #bias term \n",
    "    one = np.array([[1]])\n",
    "    #create empty matrics that will hold partial derivatives \n",
    "    thetasGrad = [np.zeros((thetas[t].shape)) for t in range(len(thetas))]\n",
    "    \n",
    "    #loop over each example one at a time, perform feedforward, back propagation, and return matrices of partial derivatives \n",
    "    for j in range(Xdata.shape[0]):\n",
    "        #initialize network archictecutre \n",
    "        network = initializeNet(64,[25],1,10)\n",
    "        #index of the output layer in the network\n",
    "        output_layer = len(network)-1\n",
    "        #list that will hold all the deltas from backprop. for each training example \n",
    "        delta_list = []\n",
    "        #set input layer of network (each row of Xdata)\n",
    "        network[0] = Xdata[[j]].T\n",
    "        #add bias unit\n",
    "        network[0] = np.append(one,network[0],axis=0)\n",
    "        \n",
    "        #forward propagation\n",
    "        for i in range(len(thetas)):\n",
    "            #use theta matrix (weights) to go from one layer to the next\n",
    "            z = np.dot(thetas[i],network[i])\n",
    "            #apply activation function\n",
    "            a = sigmoid(z)\n",
    "            #fill next layer of network\n",
    "            network[i+1] = a\n",
    "            #add bias unit\n",
    "            if i+1<output_layer:\n",
    "                network[i+1] = np.append(one,network[i+1],axis=0)\n",
    "        \n",
    "        #Backpropagation\n",
    "        #compute first delta to get error (output-actual) \n",
    "        delta = network[output_layer] - ydata[[j]].T\n",
    "        delta_list.append(delta)\n",
    "        #this loop will go backwards and compute delta(L-1),delta(L-2),..,delta(2)\n",
    "        for k in range(len(thetas)-1,0,-1):\n",
    "            delta = np.dot(thetas[k].T,delta)*sigmoidGradient(network[k])\n",
    "            delta_list.append(delta)\n",
    "            \n",
    "        #remove bias terms after computing deltas (remember output layer doesn't have a bias unit)\n",
    "        for l in range(1,len(delta_list)):\n",
    "            temp = delta_list[l]\n",
    "            delta_list[l] = temp[1:len(temp)]\n",
    "            \n",
    "        #compute the gradient of the cost function\n",
    "        delta_list = list(reversed(delta_list))\n",
    "        for m in range(len(thetasGrad)):\n",
    "            thetasGrad[m]+= np.dot(delta_list[m],network[m].T)\n",
    "            \n",
    "    #take average over all training examples \n",
    "    thetasGrad =  [(1/float(M))*thetasGrad[p] for p in range(len(thetasGrad))]\n",
    "    \n",
    "    #add regularization terms to gradients of the cost function\n",
    "    for n in range(len(thetas)):\n",
    "        #multiply all columns (except the first)  of the weight matrices by the regularization parameter\n",
    "        regTemp = [lamb*thetas[n][:,x].reshape(thetas[n].shape[0],1) for x in range(1,thetas[n].shape[1])]\n",
    "        #combine all regularizaed columns computed above into a single array\n",
    "        regTempArray = np.concatenate((regTemp),axis=1)\n",
    "        #append the original first column to the array concatenated above, this is the finished regularized weight matrix\n",
    "        reg_thetas = np.concatenate((thetas[n][:,0].reshape(thetas[n].shape[0],1),regTempArray),axis=1)\n",
    "        #add the regularization term to the theta gradient matrix\n",
    "        thetasGrad[n]+=reg_thetas\n",
    "\n",
    "    #take a single step of gradient descent (batch gradient descent)        \n",
    "    update_thetas = gradientDescent(thetas,thetasGrad,alpha)\n",
    "    \n",
    "    #return updated weights\n",
    "    return update_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes the training data, randomly initialized weights and the pre-determined architecture for a neural network\n",
    "#and performs for each training example: feedforward, then backpropagation to get the partial derivatives of the cost function\n",
    "#with repsect to each weight, then updates the weights using stochastic gradient descent. The output of this function is the \n",
    "#updated matrices of weights after 1 epoch. \n",
    "#STOCHASTIC GRADIENT DESCENT\n",
    "#this function takes inputs:\n",
    "        # Xdata- array, train data we are feeding the neural net, \n",
    "        # ydata-train data labels \n",
    "        # thetas- list, contains randomly initialized weights \n",
    "#the output of the function:\n",
    "        # thetas = list, contains updated matrices of weights after 1 epoch\n",
    "def stochastic_propagation(Xdata,ydata,thetas,alpha,lamb):\n",
    "    #number of examples \n",
    "    M = Xdata.shape[0]\n",
    "    #bias term \n",
    "    one = np.array([[1]])\n",
    "    #create empty matrics that will hold partial derivatives \n",
    "    thetasGrad = [np.zeros((thetas[t].shape)) for t in range(len(thetas))]\n",
    "    j=0\n",
    "    \n",
    "    while j<M:\n",
    "        #initialize network archictecutre \n",
    "        network = initializeNet(64,[25],1,10)\n",
    "        #index of the output layer in the network\n",
    "        output_layer = len(network)-1\n",
    "        #list that will hold all the deltas from backprop. for each training example \n",
    "        delta_list = []\n",
    "        #set input layer of network (each row of Xdata)\n",
    "        network[0] = Xdata[[j]].T\n",
    "        #add bias unit\n",
    "        network[0] = np.append(one,network[0],axis=0)\n",
    "        \n",
    "        #forward propagation\n",
    "        for i in range(len(thetas)):\n",
    "            #use theta matrix (weights) to go from one layer to the next\n",
    "            z = np.dot(thetas[i],network[i])\n",
    "            #apply activation function\n",
    "            a = sigmoid(z)\n",
    "            #fill next layer of network\n",
    "            network[i+1] = a\n",
    "            #add bias unit\n",
    "            if i+1<output_layer:\n",
    "                network[i+1] = np.append(one,network[i+1],axis=0)\n",
    "        \n",
    "        #Backpropagation\n",
    "        #compute first delta to get error (output-actual) \n",
    "        delta = network[output_layer] - ydata[[j]].T\n",
    "        delta_list.append(delta)\n",
    "        #this loop will go backwards and compute delta(L-1),delta(L-2),..,delta(2)\n",
    "        for k in range(len(thetas)-1,0,-1):\n",
    "            delta = np.dot(thetas[k].T,delta)*sigmoidGradient(network[k])\n",
    "            delta_list.append(delta)\n",
    "            \n",
    "        #remove bias terms after computing deltas (remember output layer doesn't have a bias unit)\n",
    "        for l in range(1,len(delta_list)):\n",
    "            temp = delta_list[l]\n",
    "            delta_list[l] = temp[1:len(temp)]\n",
    "            \n",
    "        #compute the gradient of the cost function\n",
    "        delta_list = list(reversed(delta_list))\n",
    "        for m in range(len(thetasGrad)):\n",
    "            thetasGrad[m] = np.dot(delta_list[m],network[m].T)  \n",
    "        \n",
    "        #add regularization terms to gradients of the cost function\n",
    "        for n in range(len(thetas)):\n",
    "            #multiply all columns (except the first)  of the weight matrices by the regularization parameter (lambda)\n",
    "            regTemp = [lamb*thetas[n][:,x].reshape(thetas[n].shape[0],1) for x in range(1,thetas[n].shape[1])]\n",
    "            #combine all regularizaed columns computed above into a single array\n",
    "            regTempArray = np.concatenate((regTemp),axis=1)\n",
    "            #append the original first column to the array concatenated above, this is the finished regularized weight matrix\n",
    "            reg_thetas = np.concatenate((thetas[n][:,0].reshape(thetas[n].shape[0],1),regTempArray),axis=1)\n",
    "            #add the regularization term to the theta gradient matrix\n",
    "            thetasGrad[n]+=reg_thetas\n",
    "\n",
    "        #take a single step of gradient descent for each training example (stochastic gradient desent)\n",
    "        update_thetas = gradientDescent(thetas,thetasGrad,alpha)\n",
    "        #update network weights \n",
    "        thetas = update_thetas\n",
    "        #update j to access the next training example\n",
    "        j+=1\n",
    "    \n",
    "    #return updated weights\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#once we have found the optimal weights for our neural network this function can be called to make predictions on the train set\n",
    "#and the test set. The inputs of this function include:\n",
    "    #Xdata- array, X-matrix data \n",
    "    #ydata- array, labels for the X-matrix\n",
    "    #network- list, in the list are arrays which contain the network architecture \n",
    "    #weights- list, in the list is are arrays containing the learned weights for the neural network\n",
    "#the output of this function is:\n",
    "    #float(count)./m- float, this is the accuracy of the network (percentage of correct predictions)\n",
    "def prediction(Xdata,ydata,network,weights):\n",
    "    #list that will hold output from feedforward\n",
    "    feedforward_outputs = []\n",
    "    #bias term \n",
    "    one = np.array([[1]])\n",
    "    #index of the output layer in the network\n",
    "    output_layer = len(network)-1\n",
    "    #list that will hold all the predictions \n",
    "    predictions = []\n",
    "    #count of all correct predictions \n",
    "    count = 0\n",
    "    #number of examples \n",
    "    m = Xdata.shape[0]\n",
    "    \n",
    "    #loop over each example one at a time and perform feedforward \n",
    "    for j in range(Xdata.shape[0]):\n",
    "        #set input layer of network (each row of Xdata)\n",
    "        network[0] = Xdata[[j]].T\n",
    "        #add bias unit\n",
    "        network[0] = np.append(one,network[0],axis=0)\n",
    "    \n",
    "        for i in range(len(weights)):\n",
    "            #use theta matrix (weights) to go from one layer to the next\n",
    "            z = np.dot(weights[i],network[i])\n",
    "            #apply activation function\n",
    "            a = sigmoid(z)\n",
    "            #fill next layer of network\n",
    "            network[i+1] = a\n",
    "            #add bias term for each hidden layer\n",
    "            if i+1<output_layer:\n",
    "                network[i+1] = np.append(one,network[i+1],axis=0)\n",
    "            #store all output-layers in a list     \n",
    "            else:\n",
    "                feedforward_outputs.append(network[output_layer])\n",
    "                \n",
    "    #loop over all outputs layers and determine the prediciton made by the neural network for each example \n",
    "    for k in range(len(feedforward_outputs)):\n",
    "        #output layer for single example \n",
    "        output = feedforward_outputs[k]\n",
    "        #maximum probability in output layer \n",
    "        max_prob = max(output)\n",
    "        #find index where maximum probability is in output layer \n",
    "        pred_d1, pred_d2 = np.where(output == max_prob)\n",
    "        #this is the correct label for the given example (recall using one-hot encoding each label is a vector)\n",
    "        label = ydata[k]\n",
    "        #the index of the correct label for the example \n",
    "        ind = np.where(label == 1)\n",
    "        if ind[0] == pred_d1[0]:\n",
    "            count+=1\n",
    "            \n",
    "    return float(count)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function will train a neural network and return the optimal weights. As inputs the function will take:\n",
    "    #X: training data\n",
    "    #y: labels for training data \n",
    "    #weights: randomly initialized weights \n",
    "    #epochs: the number of times we will make a full pass through the training data \n",
    "    #alpha: step size\n",
    "    #solver: \"stochastic\" if we want to use stochastic gradient descent and \"full_batch\" if we want to use batch gradient descent\n",
    "#the function will return:\n",
    "    #weights: the optimal weights found after the given number of epochs\n",
    "    #weight_history: a list with the updated weights after each epoch \n",
    "def trainModel(X,y,weights,epochs,alpha,lamb,solver):\n",
    "    weight_history = []\n",
    "    if solver == 'stochastic':\n",
    "        for i in range(epochs):\n",
    "            new_weights = stochastic_propagation(X,y,weights,alpha,lamb)\n",
    "            weight_history.append(new_weights)\n",
    "            weights = new_weights\n",
    "        \n",
    "    elif solver == 'full_batch':\n",
    "        for j in range(epochs):\n",
    "            new_weights = propagation(X,y,weights,alpha,lamb)\n",
    "            weight_history.append(new_weights)\n",
    "            weights = new_weights\n",
    "            \n",
    "    else:\n",
    "        return 'solver is not recognized: solvers that are supported include, stochastic and full_batch'\n",
    "            \n",
    "    return new_weights,weight_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function will perform k-fold cross validation for a given dataset using a nerual network. As inputs the function will \n",
    "#take:\n",
    "    #X - array, Xdata\n",
    "    #y - array, ydata\n",
    "    #alpha - float, step-size\n",
    "    #solver - string, type of optimization algorithm (stochastic or full_batch gradient descent)\n",
    "    #epochs - int, number of epochs each model will be trained for\n",
    "    #k - int, number of folds \n",
    "#this function will print out the accuracy for each of the k-models that are trained and return the average of those k-accuracies\n",
    "def kfoldCV(X,y,alpha,lamb,solver,epochs,k):\n",
    "    #zip together X-data and corresponding labels \n",
    "    zipdat = zip(X,y)\n",
    "    #randomly shuffle data while keeping each example and its labels matched \n",
    "    random.shuffle(zipdat)\n",
    "    #unzip X-data and labels\n",
    "    X,y = zip(*zipdat)\n",
    "    #convert X-data and labels from tuples to arrays \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    #convert each label to a vector \n",
    "    y = yLabels(y,[0,1,2,3,4,5,6,7,8,9])\n",
    "    #take transpose of y so each row represents a single example \n",
    "    y = y.T\n",
    "    #set m equal to the number of training examples \n",
    "    m = X.shape[0]\n",
    "    #this will be the size of each partition\n",
    "    partition = m/k\n",
    "    \n",
    "    #create two lists one for the Xfolds and one for the yfolds \n",
    "    Xfolds = [X[i*partition:i*partition+partition] for i in range(k)]\n",
    "    yfolds = [y[i*partition:i*partition+partition] for i in range(k)]\n",
    "    \n",
    "    #split dataset into train/test and create a model for each fold and assess each model's accuracy \n",
    "    for l in range(k):\n",
    "        #create two empty lists which will contain the X-data and labels for each of the k-folds\n",
    "        Xtemp = []\n",
    "        ytemp = []\n",
    "        #create an empty list which will hold the k accuracies for each of the k models \n",
    "        accuracies = []\n",
    "        #make the test set one of the k-folds for each model\n",
    "        Xtest_set = Xfolds[l]\n",
    "        ytest_set = yfolds[l]\n",
    "        #loop over all the folds and pull out the ones that are not the test fold for that iteration \n",
    "        for m in range(len(Xfolds)):\n",
    "            if m!=l:\n",
    "                Xtemp.append(Xfolds[m])\n",
    "                ytemp.append(yfolds[m])\n",
    "        #combine so X-data and ydata are arrays \n",
    "        Xtemp = np.concatenate((Xtemp),axis=0)\n",
    "        ytemp = np.concatenate((ytemp),axis=0)\n",
    "        \n",
    "        #initialize network and weights for the neural nets. \n",
    "        network = initializeNet(64,[25],1,10)\n",
    "        weights = initializeWeights(network)\n",
    "        #learn the optimal weights for each of the k models \n",
    "        learned_weights = trainModel(Xtemp,ytemp,weights,epochs,alpha,lamb,solver)[0]\n",
    "        #find the accuracy for each of the k-models and save in list named accuracies \n",
    "        accuracies.append(prediction(Xtest_set,ytest_set,network,learned_weights)*100)\n",
    "        #print the accuracy found for each model\n",
    "        print accuracies\n",
    "        \n",
    "        #return the average of the accuracies found for the k models trained above  \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes as inputs the hvec, which is a matrix of all the outputs for each example from the feedforward process\n",
    "#and yvec which represents the labels in vector form where 1=yes, 0=no.. for example for \n",
    "#digit recognition, y=3 is [0,0,1,0,0,0,0,0,0,0]\n",
    "#NOTE: this function does NOT include regularization\n",
    "def costFunction(hvec,yvec):\n",
    "    J = 0\n",
    "    m = hvec.shape[0]\n",
    "    hvec = np.asarray(hvec)\n",
    "    yvec = np.asarray(yvec)\n",
    "    for i in range(0,hvec.shape[0]):\n",
    "        J = J + (1./m)*(sum((-yvec[:,i].T*np.log(hvec[i,:]))-((1-yvec[:,i].T)*(np.log(1-hvec[i,:])))))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load Xdata\n",
    "Xdata = digits['data']\n",
    "#load labels\n",
    "ydata = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one hot encoding for outputs \n",
    "ylabels = yLabels(ydata,[0,1,2,3,4,5,6,7,8,9])\n",
    "#each column corresponds to a single example\n",
    "y = ylabels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize network architecture, 64 features, 1 hidden layer with 25 hidden units, output layer with 10 classes \n",
    "network = initializeNet(64,[25],1,10)\n",
    "#randomly initialize network weights according to the network architecutre\n",
    "thetas = initializeWeights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data set into training set and test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 43.97%\n",
      "Train Accuracy: 64.84%\n",
      "Train Accuracy: 75.98%\n",
      "Train Accuracy: 80.13%\n",
      "Train Accuracy: 82.96%\n",
      "Train Accuracy: 83.46%\n",
      "Train Accuracy: 85.37%\n",
      "Train Accuracy: 86.37%\n",
      "Train Accuracy: 87.45%\n",
      "Train Accuracy: 87.20%\n",
      "Train Accuracy: 89.36%\n",
      "Train Accuracy: 89.94%\n",
      "Train Accuracy: 90.27%\n",
      "Train Accuracy: 91.69%\n",
      "Train Accuracy: 92.19%\n",
      "Train Accuracy: 92.10%\n",
      "Train Accuracy: 92.19%\n",
      "Train Accuracy: 93.18%\n",
      "Train Accuracy: 92.85%\n",
      "Train Accuracy: 93.43%\n",
      "Train Accuracy: 94.18%\n",
      "Train Accuracy: 93.43%\n",
      "Train Accuracy: 94.18%\n",
      "Train Accuracy: 94.35%\n",
      "Train Accuracy: 94.68%\n",
      "Train Accuracy: 95.01%\n",
      "Train Accuracy: 94.93%\n",
      "Train Accuracy: 95.01%\n",
      "Train Accuracy: 95.76%\n",
      "Train Accuracy: 95.76%\n",
      "Train Accuracy: 95.93%\n",
      "Train Accuracy: 96.18%\n",
      "Train Accuracy: 95.59%\n",
      "Train Accuracy: 96.18%\n",
      "Train Accuracy: 96.26%\n",
      "Train Accuracy: 96.76%\n",
      "Train Accuracy: 96.67%\n",
      "Train Accuracy: 96.59%\n",
      "Train Accuracy: 96.01%\n",
      "Train Accuracy: 96.51%\n",
      "Train Accuracy: 96.34%\n",
      "Train Accuracy: 96.43%\n",
      "Train Accuracy: 97.09%\n",
      "Train Accuracy: 97.01%\n",
      "Train Accuracy: 96.92%\n",
      "Train Accuracy: 96.92%\n",
      "Train Accuracy: 96.92%\n",
      "Train Accuracy: 96.59%\n",
      "Train Accuracy: 96.84%\n",
      "Train Accuracy: 97.09%\n"
     ]
    }
   ],
   "source": [
    "#train neural network for 50 epochs using stochastic gradient descent, regularization parameter lambda=0.0001. Print \n",
    "#training accuracy for each epoch. \n",
    "for i in range(50):\n",
    "    new = stochastic_propagation(Xtrain,ytrain,thetas,0.010,0.0001)\n",
    "    print(\"Train Accuracy: {:.2f}%\".format(prediction(Xtrain,ytrain,network,new)*100))\n",
    "    thetas = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.94%\n"
     ]
    }
   ],
   "source": [
    "#print test accuracy from optimal weights found above\n",
    "print(\"Test Accuracy: {:.2f}%\".format(prediction(Xtest,ytest,network,new)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88.3008356545961]\n",
      "[92.75766016713092]\n",
      "[94.42896935933148]\n",
      "[94.70752089136491]\n",
      "[94.15041782729804]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.150417827298043"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use 5-fold cross validation and take average of the 5 accurcies\n",
    "kfoldCV(Xdata,ydata,0.01,0.00001,'stochastic',50,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
